{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2508632,"sourceType":"datasetVersion","datasetId":1519260},{"sourceId":6480693,"sourceType":"datasetVersion","datasetId":3744205}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-20T08:12:55.413411Z","iopub.execute_input":"2023-09-20T08:12:55.413988Z","iopub.status.idle":"2023-09-20T08:12:56.393912Z","shell.execute_reply.started":"2023-09-20T08:12:55.41395Z","shell.execute_reply":"2023-09-20T08:12:56.393002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:00:47.579144Z","iopub.status.idle":"2024-09-21T09:00:47.579904Z","shell.execute_reply.started":"2024-09-21T09:00:47.579652Z","shell.execute_reply":"2024-09-21T09:00:47.579675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install PyPDF2","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:00:47.576809Z","iopub.status.idle":"2024-09-21T09:00:47.577216Z","shell.execute_reply.started":"2024-09-21T09:00:47.577040Z","shell.execute_reply":"2024-09-21T09:00:47.577057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install pdfplumber\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n!python -m nltk.downloader stopwords punkt\n\n!python -m spacy download en_core_web_sm","metadata":{"execution":{"iopub.status.busy":"2024-09-21T08:48:49.280866Z","iopub.execute_input":"2024-09-21T08:48:49.281559Z","iopub.status.idle":"2024-09-21T08:49:23.226759Z","shell.execute_reply.started":"2024-09-21T08:48:49.281530Z","shell.execute_reply":"2024-09-21T08:49:23.225521Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n  warn(RuntimeWarning(msg))\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nCollecting en-core-web-sm==3.6.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /opt/conda/lib/python3.10/site-packages (from en-core-web-sm==3.6.0) (3.6.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\nRequirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.1)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.9)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (68.0.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.6.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n","output_type":"stream"}]},{"cell_type":"code","source":"# from sklearn.metrics.pairwise import cosine_similarity\n# import numpy as np\n\n# def find_best_matches(resume_embeddings, job_embedding):\n#     similarities = cosine_similarity(job_embedding, resume_embeddings)\n#     top_k_indices = np.argsort(similarities[0])[::-1][:5] \n#     return top_k_indices, similarities[0][top_k_indices]","metadata":{"execution":{"iopub.status.busy":"2024-09-21T08:50:05.716403Z","iopub.execute_input":"2024-09-21T08:50:05.716788Z","iopub.status.idle":"2024-09-21T08:50:05.722623Z","shell.execute_reply.started":"2024-09-21T08:50:05.716756Z","shell.execute_reply":"2024-09-21T08:50:05.721541Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# from sklearn.metrics.pairwise import cosine_similarity\n# import numpy as np\n\n# def find_best_matches(resume_embeddings, job_embedding):\n#     # Ensure job_embedding is 2D\n#     if job_embedding.ndim == 1:\n#         job_embedding = job_embedding.reshape(1, -1)\n   \n    \n#     similarities = cosine_similarity(job_embedding, resume_embeddings)\n#     top_k_indices = np.argsort(similarities[0])[::-1][:5] \n#     return top_k_indices, similarities[0][top_k_indices]\n\n# def filter_and_rank_candidates(parsed_query, resume_entities, resume_embeddings, job_embedding):\n#     matched_indices, similarity_scores = find_best_matches(resume_embeddings, job_embedding)\n#     filtered_candidates = []\n    \n#     for i in matched_indices:\n#         candidate = resume_entities[i]\n#         print(f\"Candidate degrees: {candidate.get('degree', 'No degree found')}\")  # Use get to avoid KeyError\n        \n#         # Check if candidate degree is not empty and matches parsed query\n#         if candidate.get('degree') and parsed_query['degree'] in candidate['degree']:\n#             filtered_candidates.append((i, similarity_scores[i], candidate))\n    \n#     filtered_candidates = sorted(filtered_candidates, key=lambda x: x[1], reverse=True)\n#     return filtered_candidates\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T08:54:38.742208Z","iopub.execute_input":"2024-09-21T08:54:38.743077Z","iopub.status.idle":"2024-09-21T08:54:38.751138Z","shell.execute_reply.started":"2024-09-21T08:54:38.743043Z","shell.execute_reply":"2024-09-21T08:54:38.750157Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Without Error ","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom pypdf import PdfReader\nfrom nltk import pos_tag, word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nimport re\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-09-21T14:34:47.030187Z","iopub.execute_input":"2024-09-21T14:34:47.030849Z","iopub.status.idle":"2024-09-21T14:34:54.301072Z","shell.execute_reply.started":"2024-09-21T14:34:47.030820Z","shell.execute_reply":"2024-09-21T14:34:54.300263Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef extract_text_from_pdf(file_path):\n    \"\"\"Extract text from the provided PDF file.\"\"\"\n    reader = PdfReader(file_path)\n    text = \"\".join(page.extract_text() for page in reader.pages)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-09-21T14:34:54.302579Z","iopub.execute_input":"2024-09-21T14:34:54.302998Z","iopub.status.idle":"2024-09-21T14:34:54.307944Z","shell.execute_reply.started":"2024-09-21T14:34:54.302971Z","shell.execute_reply":"2024-09-21T14:34:54.307079Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    \"\"\"Preprocess the text: lowercasing, removing punctuation, stop words, etc.\"\"\"\n    text = text.lower()\n    text = re.sub('[^a-zA-Z]', ' ', text) \n    sentences = sent_tokenize(text)\n    features = ''\n    \n    stop_words = set(stopwords.words(\"english\"))\n    for sent in sentences:\n        words = word_tokenize(sent)\n        words = [word for word in words if word not in stop_words]\n        tagged_words = pos_tag(words)\n        filtered_words = [word for word, tag in tagged_words if tag not in ['DT', 'IN', 'TO', 'PRP', 'WP']]\n        features += \" \".join(filtered_words) + \" \"\n    return features\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T14:34:54.309266Z","iopub.execute_input":"2024-09-21T14:34:54.309554Z","iopub.status.idle":"2024-09-21T14:34:54.319779Z","shell.execute_reply.started":"2024-09-21T14:34:54.309522Z","shell.execute_reply":"2024-09-21T14:34:54.318829Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\ndef process_resume_data(resume_file_path):\n    \"\"\"Process resumes by extracting text and preprocessing it.\"\"\"\n    text = extract_text_from_pdf(resume_file_path)\n    features = preprocess_text(text)\n    return features","metadata":{"execution":{"iopub.status.busy":"2024-09-21T14:34:54.322249Z","iopub.execute_input":"2024-09-21T14:34:54.322838Z","iopub.status.idle":"2024-09-21T14:34:54.329741Z","shell.execute_reply.started":"2024-09-21T14:34:54.322805Z","shell.execute_reply":"2024-09-21T14:34:54.328890Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_embeddings(text, model, tokenizer, device):\n    \"\"\"Generate embeddings for the given text using the model.\"\"\"\n    inputs = tokenizer(str(text), return_tensors=\"pt\", truncation=True, padding=True).to(device)\n    with torch.no_grad(): \n        outputs = model(**inputs)\n    \n    \n    embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n    \n    return embeddings","metadata":{"execution":{"iopub.status.busy":"2024-09-21T14:34:54.330852Z","iopub.execute_input":"2024-09-21T14:34:54.331131Z","iopub.status.idle":"2024-09-21T14:34:54.339717Z","shell.execute_reply.started":"2024-09-21T14:34:54.331106Z","shell.execute_reply":"2024-09-21T14:34:54.338919Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\ndef contextual_search(query, resume_data, model, tokenizer, device, top_k=5):\n    \"\"\"Perform contextual search using the query against resume data.\"\"\"\n   \n    query_features = preprocess_text(query)\n    query_embedding = get_embeddings(query_features, model, tokenizer, device)\n    \n   \n    resume_embeddings = np.vstack([get_embeddings(text, model, tokenizer, device) for text in resume_data['Feature']])\n    \n    \n    similarities = cosine_similarity(query_embedding, resume_embeddings)[0]\n    \n    \n    top_indices = np.argsort(similarities)[::-1][:top_k]\n   \n    return resume_data.iloc[top_indices], similarities[top_indices]","metadata":{"execution":{"iopub.status.busy":"2024-09-21T14:34:54.340713Z","iopub.execute_input":"2024-09-21T14:34:54.340968Z","iopub.status.idle":"2024-09-21T14:34:54.350535Z","shell.execute_reply.started":"2024-09-21T14:34:54.340941Z","shell.execute_reply":"2024-09-21T14:34:54.349673Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\ndef main():\n    resume_dir = \"/kaggle/input/resume-dataset/data/data/INFORMATION-TECHNOLOGY\"  \n    \n    resume_data = pd.DataFrame(columns=[\"ID\", \"Feature\"])\n    \n    for idx, file_name in enumerate(os.listdir(resume_dir)):\n        if file_name.endswith(\".pdf\"):\n            resume_file_path = os.path.join(resume_dir, file_name)\n            features = process_resume_data(resume_file_path)\n            resume_data = pd.concat([resume_data, pd.DataFrame({\"ID\": [file_name], \"Feature\": [features]})], ignore_index=True)\n\n    \n    model_name = \"bert-base-uncased\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModel.from_pretrained(model_name)\n    model.to(device)\n    \n    hr_query = input(\"Enter the query: \")\n    \n    \n    top_resumes, scores = contextual_search(hr_query, resume_data, model, tokenizer, device, top_k=5)\n    \n    \n    for idx, score in enumerate(scores):\n        print(f\"\\nCandidate {idx+1}:\")\n        print(f\"Resume File: {top_resumes['ID'].iloc[idx]}\")\n        print(f\"Similarity Score: {score:.4f}\")\n   \nif __name__ == \"__main__\":\n    main()\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T14:39:21.729578Z","iopub.execute_input":"2024-09-21T14:39:21.730718Z","iopub.status.idle":"2024-09-21T14:40:41.183388Z","shell.execute_reply.started":"2024-09-21T14:39:21.730673Z","shell.execute_reply":"2024-09-21T14:40:41.180092Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter the query:  8+ years of full stack development experience with exposure to node.js and azure, excellent verbal and written communication skills\n"},{"name":"stdout","text":"\nCandidate 1:\nResume File: 40018190.pdf\nSimilarity Score: 0.7056\n\nCandidate 2:\nResume File: 24083609.pdf\nSimilarity Score: 0.7004\n\nCandidate 3:\nResume File: 68460556.pdf\nSimilarity Score: 0.6991\n\nCandidate 4:\nResume File: 20024870.pdf\nSimilarity Score: 0.6935\n\nCandidate 5:\nResume File: 12635195.pdf\nSimilarity Score: 0.6882\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Transformer Approach ","metadata":{}}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSQL\n",
    "It's imp cause it includes the DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe API extends the RDD into a \"DataFrame\" object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame:\n",
    "     Conatain Row Objects\n",
    "     Can run SQL queries\n",
    "     Can have a Schema( leading to more efficient storage)\n",
    "     Read and write json,csv, Hive,parquet\n",
    "     Communicate with JDBC/ODBC , tableau\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Dataframe -- Spark SQL\n",
    "\n",
    "from pyspark.sql import SparkSession,Row\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()  # get used to connect to already made session as oppose to creating one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninputData = spark.read.json(Datafilename)  --inputData is a Dataframe \\n\\n--Now to create a Temporary view on the Dataframe to make it look like a DataBase\\n\\ninputData.createOrReplaceTempView(\"TableName\")\\n\\nmyResultsDF = spark.sql(\"Select * from bar Order by ....\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "inputData = spark.read.json(Datafilename)  --inputData is a Dataframe \n",
    "\n",
    "--Now to create a Temporary view on the Dataframe to make it look like a DataBase\n",
    "\n",
    "inputData.createOrReplaceTempView(\"TableName\")\n",
    "\n",
    "myResultsDF = spark.sql(\"Select * from bar Order by ....\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' example:\\nmydf.show()\\nmydf.select(\"somefieldName\")\\nmydf.filter(mydf(\"someFieldName\">200))\\nmydf.groupBy(mydf(\"somefieldname\")).mean()\\nmydf.rdd().map(function)\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dont have to use SQL language specifically , can call methods on df directly that emulates SQL\n",
    "\n",
    "''' example:\n",
    "mydf.show()\n",
    "mydf.select(\"somefieldName\")\n",
    "mydf.filter(mydf(\"someFieldName\">200))\n",
    "mydf.groupBy(mydf(\"somefieldname\")).mean()\n",
    "mydf.rdd().map(function)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstart with sbin/start-thriftserver.sh\\nlisten on port 10000 by default \\nConnect using bin/beeline -u jdbc:hive2://localhost:10000\\n-- gives you sql shell to sparksql\\nCan creat new tables or query existing ones that were cached using\\nhiveCtx.cacheTable(\"tableName\")\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DF is really a DataSet of Row Objects\n",
    "# SparkSQL EXPOSES a JDBC/ODBC server -- if you build spark with hive support\n",
    "'''\n",
    "start with sbin/start-thriftserver.sh\n",
    "listen on port 10000 by default \n",
    "Connect using bin/beeline -u jdbc:hive2://localhost:10000\n",
    "-- gives you sql shell to sparksql\n",
    "Can creat new tables or query existing ones that were cached using\n",
    "hiveCtx.cacheTable(\"tableName\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.square(x)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def square(x):\n",
    "    return x*x\n",
    "\n",
    "spark.udf.register(\"square\",square,IntegerType())\n",
    "# spark.udf.register(\"function name you wanna give\",point to func,datatype func takes)\n",
    "\n",
    "# df = spark.sql(\"Select square['some numerical field] from TableName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To stop the session after work\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
